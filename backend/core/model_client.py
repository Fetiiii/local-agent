import ollama
from typing import List, Dict, AsyncGenerator, Any, Optional, Union
import json
import asyncio

class ModelClient:
    def __init__(self, model_name: str = "glm4.7-flash:latest"):
        self.model_name = model_name
        self.client = ollama.AsyncClient()
        print(f"ðŸ¤– Model Client HazÄ±r: {self.model_name}")

    async def generate(self, messages: List[Dict[str, str]], stream: bool = True, json_mode: bool = False) -> Union[AsyncGenerator[str, None], str]:
        """
        Ollama Chat API'sini Ã§aÄŸÄ±rÄ±r (Async).
        
        Args:
            messages: [{"role": "user", "content": "..."}] formatÄ±nda
            stream: True ise AsyncGenerator dÃ¶ner, False ise string.
            json_mode: True ise Ã§Ä±ktÄ± JSON'a zorlanÄ±r.
        """
        
        options = {
            "temperature": 0.7,
            "num_ctx": 8192, # Context window artÄ±rÄ±ldÄ±
        }
        
        format_param = "json" if json_mode else None

        try:
            if stream:
                return self._stream_generator(messages, options, format_param)
            else:
                response = await self.client.chat(
                    model=self.model_name,
                    messages=messages,
                    options=options,
                    format=format_param,
                    stream=False
                )
                return response['message']['content']
                
        except Exception as e:
            # Ollama JSON parse hatasÄ± verirse (model JSON formatÄ±na uyamazsa)
            # json_mode olmadan tekrar denemeyi veya hatayÄ± yÃ¶netmeyi saÄŸlar.
            if "parsing" in str(e).lower() and json_mode:
                print(f"âš ï¸ Ollama JSON Parse HatasÄ±: {e}. Raw moda dÃ¶nÃ¼lÃ¼yor...")
                # Fallback durumunda model_name'i aÃ§Ä±kÃ§a belirt
                response = await self.client.chat(
                    model=self.model_name,
                    messages=messages,
                    options=options,
                    stream=False
                )
                return response['message']['content']
            
            return f"Error communicating with Ollama: {str(e)}"

    async def _stream_generator(self, messages, options, format_param) -> AsyncGenerator[str, None]:
        stream = await self.client.chat(
            model=self.model_name,
            messages=messages,
            options=options,
            format=format_param,
            stream=True
        )
        
        async for chunk in stream:
            content = chunk['message']['content']
            if content:
                yield content

    async def check_connection(self) -> bool:
        try:
            await self.client.list()
            return True
        except:
            return False